
cd ~/dz/-Diplom/bootstrap

export YC_TOKEN=$(yc iam create-token)
export AWS_ACCESS_KEY_ID=$(terraform output -raw sa_access_key)
export AWS_SECRET_ACCESS_KEY=$(terraform output -raw sa_secret_key)
export AWS_DEFAULT_REGION="ru-central1"

cd ~/dz/-Diplom/infra

terraform init -reconfigure
terraform destroy
terraform apply



export REGISTRY_ID=$(terraform output -raw container_registry_id)

terraform apply -var "ssh_public_key=$(cat ~/.ssh/id_ed25519.pub)"









Обновляем inventory Kubespray
На локальной машине:
cd ~/kubespray
nano inventory/diplom-cluster/hosts.yaml
Прописываю актульные ip



 # задеплоить кластер Kubespray
cd ~/kubespray
source venv/bin/activate
ansible-playbook -i inventory/diplom-cluster/hosts.yaml cluster.yml -b -v

Разворачиваю

копирую на мастер ноду



На Мастере:

sudo -i
mkdir -p /root/.kube
cp /etc/kubernetes/admin.conf /root/.kube/config
chmod 600 /root/.kube/config


kubectl get nodes



На master:

ssh ubuntu@<public-ip-master>
sudo -i
cp /etc/kubernetes/admin.conf ~/.kube/config



# посмотреть внутренние IP нод
kubectl get nodes -o wide


///////////////////////////////////////////////
На мастере: подготовить kubeconfig
ssh ubuntu@<master_public_ip>
sudo -i

# копируем актуальный admin.conf в домашний каталог ubuntu
cp /etc/kubernetes/admin.conf /home/ubuntu/diplom-kubeconfig.yaml
chown ubuntu:ubuntu /home/ubuntu/diplom-kubeconfig.yaml
exit

Скопировать на локальную машину
На своей локальной машине:
scp ubuntu@<master_public_ip>:/home/ubuntu/diplom-kubeconfig.yaml ~/.kube/diplom-kubeconfig.yaml
chmod 600 ~/.kube/diplom-kubeconfig.yaml

ssh -L 6443:127.0.0.1:6443 ubuntu@84.252.128.124 # Проброс сессии на локальном пк

Теперь в другом локальном терминале:
Проверка API

export KUBECONFIG=$HOME/.kube/diplom-kubeconfig.yaml
kubectl get nodes
kubectl get pods -A





    
# kubectl -n monitoring port-forward svc/grafana 3000:3000
# http://localhost:3000/
